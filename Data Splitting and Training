###   STEP :- 7 (Test and Train Data Split)   ###


#   Splitting data into Train and Testing


# (Function to convert character or factor columns to integers)

object_to_int <- function(column) {
  if (is.character(column) || is.factor(column)) {
    column <- as.numeric(as.factor(column)) - 1  # (Convert to numeric and subtract 1 to start from 0)
  }
  return(column)
}

SURdf <- SURdf %>%
  mutate(across(where(is.character), object_to_int))  # (Applying to all character columns)


# (Define the object_to_int function)

object_to_int <- function(column) {
  if (is.character(column) || is.factor(column)) {
    column <- as.numeric(as.factor(column)) - 1  # (Converting to numeric starting from 0)
  }
  return(column)
}

SURdf <- as.data.frame(lapply(SURdf, object_to_int))

head(SURdf)


# (Ensuring 'Churn' is numeric for correlation calculation)

if (!is.numeric(SURdf$Churn)) {
  SURdf$Churn <- as.numeric(as.factor(SURdf$Churn)) - 1 
}

                                       # (Select only numeric columns for correlation calculation)
numeric_cols <- sapply(SURdf, is.numeric)
numeric_df <- SURdf[, numeric_cols]  

                                        # (Calculate correlations for numeric columns)
correlations <- cor(numeric_df, use = "pairwise.complete.obs")

                                         # (Extract correlations with 'Churn')
if ("Churn" %in% colnames(correlations)) {
  churn_correlations <- correlations[, "Churn"]
  
  sorted_correlations <- sort(churn_correlations, decreasing = TRUE)

  print(sorted_correlations)
} else {
  print("The 'Churn' column is not in the correlation matrix. Check column names or Data Preprocessing!!!")
}


# (Defining the dependent variable (y) as 'Churn' and independent variables (X) by dropping 'Churn')

y <- SURdf$Churn  

X <- SURdf %>% select(-Churn)


# (Combining X and y into a single dataframe for stratification)

data <- cbind(X, Churn = y)

                                                   # (Random seed for reproducibility)
set.seed(40)

                                      # (Split the data into training (70%) and testing (30%) sets)
trainIndex <- createDataPartition(data$Churn, p = 0.7, list = FALSE, times = 1)

                                            # (I made training and testing datasets)
trainData <- data[trainIndex, ]             # (Training set)
testData <- data[-trainIndex, ]             # (Testing set)

                               # (Separate predictors (X) and target (y) in training and testing sets)
X_train <- trainData %>% select(-Churn)
y_train <- trainData$Churn
X_test <- testData %>% select(-Churn)
y_test <- testData$Churn


#  DISTPLOT FUNCTION

distplot <- function(feature, frame, color = "yellow") {
  ggplot(frame, aes_string(x = feature)) +
    geom_density(fill = color, alpha = 0.5) +
    labs(
      title = paste("Distribution for", feature),
      x = feature,
      y = "Density"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      axis.title = element_text(size = 12),
      axis.text = element_text(size = 10)
    )
}

distplot("MonthlyCharges", SURdf, color = "blue")
distplot("tenure", SURdf, color = "green")
distplot("TotalCharges", SURdf, color = "gold")


#   Since the numerical features are distributed over different value ranges, I will use standard scalar to scale them down to the same range.


# (Standardizing numerical columns)


numerical_cols <- c("MonthlyCharges", "TotalCharges", "tenure")

df_std <- as.data.frame(scale(SURdf[numerical_cols]))  # (Standardize and convert to dataframe)
colnames(df_std) <- numerical_cols

distplot <- function(feature, frame, color = "cyan") {
  ggplot(frame, aes_string(x = feature)) +
    geom_density(fill = color, alpha = 0.5) + 
    labs(
      title = paste("Distribution for", feature),
      x = feature,
      y = "Density"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      axis.title = element_text(size = 12),
      axis.text = element_text(size = 10)
    )
}

for (feat in numerical_cols) {
  print(distplot(feat, df_std, color = "cyan"))
}


#  (STANDARDIZATION, One-Hot ENCODING, LABEL ENCODING)

numerical_cols <- c("MonthlyCharges", "TotalCharges", "tenure") 

                                                                   # (Define the categorical columns for one-hot encoding)
cat_cols_ohe <- c("PaymentMethod", "Contract", "InternetService")  # (Categorical columns for one-hot encoding)

                                                                   # (Those that need label encoding)
cat_cols_le <- setdiff(
  colnames(SURdf),                 
  c(numerical_cols, cat_cols_ohe)                                  # (Exclude numerical and one-hot encoding columns)
)

SURdf[cat_cols_le] <- lapply(SURdf[cat_cols_le], as.factor)

cat_cols_ohe   
cat_cols_le    
numerical_cols


#  (Ensuring the required numerical columns are defined)

numerical_cols <- c("MonthlyCharges", "TotalCharges", "tenure") 

                                                 # Standardize numerical columns in the training set
X_train[numerical_cols] <- as.data.frame(scale(X_train[numerical_cols]))

                                                 # Standardize numerical columns in the test set using training data statistics
train_means <- colMeans(X_train[numerical_cols])
train_sds <- apply(X_train[numerical_cols], 2, sd)
X_test[numerical_cols] <- as.data.frame(
  scale(X_test[numerical_cols], center = train_means, scale = train_sds)
)

                                                 # View the transformed datasets
head(X_train[numerical_cols])              # Standardized training set
head(X_test[numerical_cols])               # Standardized test set
